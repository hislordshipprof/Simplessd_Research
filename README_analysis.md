# RL-GC Analysis Tool

This tool analyzes the log files generated by the RL-GC (Reinforcement Learning-based Garbage Collection) system to extract meaningful insights about its performance.

## Features

The analysis script extracts and visualizes the following metrics:

1. **Long-Tail Latency**
   - Calculates percentiles (50th, 70th, 90th, 95th, 99th, 99.9th, 99.99th, 99.999th, 99.9999th)
   - Visualizes response time distribution

2. **Average Latency**
   - Calculates average read and write latency
   - Provides detailed statistics on response times

3. **Erase Count and GC Efficiency**
   - Tracks total erase operations
   - Analyzes page copies during garbage collection
   - Calculates efficiency metrics like average pages copied per GC

4. **Free Block Dynamics**
   - Visualizes free block count at GC trigger points
   - Analyzes distribution of free blocks during GC operations

5. **RL Learning Progress**
   - Tracks Q-value evolution for top state-action pairs
   - Visualizes epsilon decay over time
   - Analyzes reward distribution

## Requirements

- Python 3.6+
- NumPy
- Matplotlib

Install the required packages:

```bash
pip install numpy matplotlib
```

## Usage

Run the analysis script with default parameters:

```bash
python analyze_rlgc.py
```

### Command-line Arguments

- `--rlgc_log`: Path to RL-GC debug log (default: 'output/rl_gc_debug.log')
- `--ftl_log`: Path to FTL debug log (default: 'output/ftl_debug.log')
- `--general_log`: Path to general log (default: 'output/log.txt')
- `--output_dir`: Directory to save analysis results (default: 'analysis_results')

Example with custom paths:

```bash
python analyze_rlgc.py --rlgc_log custom/path/rl_gc_debug.log --output_dir my_results
```

## Output

The script generates:

1. **Text Report**: A comprehensive analysis report in `analysis_results/analysis_results.txt`
2. **Visualizations**: Several plots saved as PNG files in the output directory:
   - Response time distribution (normal and log scale)
   - Free blocks at GC trigger points
   - Q-value evolution for top state-action pairs
   - Epsilon decay over time
   - Reward distribution

## Interpreting the Results

### Tail Latency
Lower percentile values indicate better performance. The 99.9999th percentile represents the worst-case scenario latency.

### GC Efficiency
- Higher reclaimed blocks per GC operation indicate more efficient garbage collection
- Lower page copies per reclaimed block indicate better efficiency

### RL Learning Progress
- Convergence of Q-values indicates the learning algorithm is stabilizing
- Decreasing epsilon values show the transition from exploration to exploitation
- High average rewards indicate the RL system is making good decisions

## Example Analysis

Here's an example of what you might find in the analysis:

```
=== Response Time Statistics ===
Total response times recorded: 10000
Min response time: 100000 ns
Max response time: 5000000 ns
Mean response time: 250000.00 ns
Median response time: 200000.00 ns

=== Tail Latency ===
50th percentile: 200000.00 ns
70th percentile: 250000.00 ns
90th percentile: 350000.00 ns
99th percentile: 1000000.00 ns
99.9th percentile: 2500000.00 ns
99.99th percentile: 3500000.00 ns
99.999th percentile: 4500000.00 ns
99.9999th percentile: 4900000.00 ns
```

This would indicate that while most operations complete within 350,000 ns (90th percentile), there are some outliers that take significantly longer (up to 4,900,000 ns for the 99.9999th percentile). 